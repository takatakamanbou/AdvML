{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/AdvML/blob/2024/AdvML2024_ex11notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# AdvML ex11notebookA\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/AdvML/AdvML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?AdvML)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "板書や口頭で補足する前提なので，この notebook だけでは説明が不完全です．\n"
      ],
      "metadata": {
        "id": "bStHGoTvH0gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 混合正規分布モデルとEMアルゴリズム\n",
        "---\n"
      ],
      "metadata": {
        "id": "qrKE45oj_dUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 準備\n"
      ],
      "metadata": {
        "id": "zl03icnhjxTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.mixture import GaussianMixture"
      ],
      "metadata": {
        "id": "CY8ptRIEj3eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy の 疑似乱数生成器（rng = random number generator）\n",
        "from numpy.random import default_rng\n",
        "rng = default_rng() # 疑似乱数生成器を初期化"
      ],
      "metadata": {
        "id": "py6LNueHhMu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 混合正規分布モデル"
      ],
      "metadata": {
        "id": "c7ZbKW4L6lqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 単一の正規分布ではうまく表せそうにないデータ"
      ],
      "metadata": {
        "id": "dAMkmNL_s_V6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Old Faithful Geyser Dataset という2次元のデータ．\n",
        "これは，米国イエローストーン国立公園内にある，「[The Old Faithful Geyser](https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%BC%E3%83%AB%E3%83%89%E3%83%BB%E3%83%95%E3%82%A7%E3%82%A4%E3%82%B9%E3%83%95%E3%83%AB%E3%83%BB%E3%82%AC%E3%82%A4%E3%82%B6%E3%83%BC)」\n",
        "という間欠泉（一定周期で熱湯の噴出と停止を繰り返す）を観測して得られたデータ．\n",
        "\n",
        "ここでは，以下の URL から取得したものを用いる．\n",
        "\n",
        "https://gist.github.com/curran/4b59d1046d9e66f2787780ad51a1cd87\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9G9DblpEtZXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'https://gist.github.com/curran/4b59d1046d9e66f2787780ad51a1cd87/raw/9ec906b78a98cf300947a37b56cfe70d01183200/data.tsv'\n",
        "dfOFG = pd.read_csv(URL, sep='\\t')\n",
        "XOFG = dfOFG.to_numpy()\n",
        "print(XOFG.shape)\n",
        "dfOFG"
      ],
      "metadata": {
        "id": "4iGCACyhuKQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "eruptions は噴出の持続時間[分]，waiting は次の噴出までの時間[分]．\n",
        "散布図は次の通り．"
      ],
      "metadata": {
        "id": "u5_Ob9zzua1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# グラフを描く\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(dfOFG['eruptions'], dfOFG['waiting'])\n",
        "xmin, xmax = 0, 6\n",
        "ymin, ymax = 40, 100\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_xlabel('Eruptions [minutes]')\n",
        "ax.set_ylabel('Waiting [minutes]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KGojfa1EuiDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この2次元のデータに一つの正規分布を当てはめるのはよい考えとは言えなさそうである．\n",
        "この例に限らず，世の中には単一の正規分布ではうまくモデル化できない．どうしよう？"
      ],
      "metadata": {
        "id": "9jcWY4myvgI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 混合正規分布モデル"
      ],
      "metadata": {
        "id": "A-BPr47gv_NB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**混合正規分布モデル** (Gaussian Mixture Model, GMM) は，複数の多次元正規分布を組み合わせて複雑な分布を表現するものである．\n",
        "\n",
        "$K$ 個の正規分布 ${\\cal N}(\\mathbf{x}; \\mathbf{\\mu}_k, \\Sigma_k)$ を用いて，個々のデータが次のようにして生成されるとする:\n",
        "\n",
        "1. 確率 $w_k$ で $k$ 番目の正規分布が選ばれる． $0 \\leq w_k \\leq 1$ ($k = 1, 2, \\ldots, K$) かつ $\\sum_{k=1}^{K}w_k = 1$ とする．\n",
        "1. その正規分布に従ってデータが生成される\n",
        "\n"
      ],
      "metadata": {
        "id": "powdaAj6wMA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GMM を式で表そう．\n",
        "まず，どの正規分布を選ぶかを表す確率変数 $z$ を導入する．\n",
        "$z$ は $1, 2, \\ldots, K$ のいずれかをとる離散確率変数である．\n",
        "すると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(z = k) &= w_k\\\\\n",
        "p(\\mathbf{x}|z = k) &= {\\cal N}(\\mathbf{x}; \\mathbf{\\mu}_k, \\Sigma_k)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "と表せる．このとき，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(\\mathbf{x}) = \\sum_{k=1}^{K} p(\\mathbf{x}|z = k) p(z = k)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "なので，$p(\\mathbf{x})$ は次式のように表される．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(\\mathbf{x}) = \\sum_{k=1}^{K} w_k {\\cal N}(\\mathbf{x}; \\mathbf{\\mu}_k, \\Sigma_k)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "これが GMM の式である．\n",
        "GMM のパラメータは，$w_k, \\mathbf{\\mu}_k, \\Sigma_k$ ($k = 1, 2, \\ldots, K$) である．\n",
        "\n"
      ],
      "metadata": {
        "id": "sDGM3WODbOC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記の導出過程では， $z$ という確率変数が登場した．\n",
        "確率変数 $\\mathbf{x}$ が観測できる（データが直接手に入る）のに対して，この確率変数 $z$ は観測できるものではない．\n",
        "GMM において，データの生成過程に隠れて存在していると仮定した変数である．\n",
        "このような変数のことを **潜在変数** (latent variable) という．\n",
        "GMM は，潜在変数を持つ確率モデルの代表例である．\n"
      ],
      "metadata": {
        "id": "EXqqO1bPyvDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 実験: Old Faithful Dataset に GMM を当てはめてみる"
      ],
      "metadata": {
        "id": "ua4JXJSY0Es-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GMM のパラメータをどうやって推定するかという話は後回しにして， The Old Faithful Dataset Geyser Dataset に GMM を当てはめてみよう．\n",
        "ここでは， [sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) を用いる．"
      ],
      "metadata": {
        "id": "L7_WLRo80PLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GMM の当てはめ\n",
        "K = 2\n",
        "gmm = GaussianMixture(n_components=K)\n",
        "gmm.fit(XOFG)"
      ],
      "metadata": {
        "id": "8kusCa4x04H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 推定されたモデルパラメータの表示\n",
        "for k in range(K):\n",
        "    print(f'### {k}-th component')\n",
        "    print('weight = ', gmm.weights_[k])\n",
        "    print('mu = ', gmm.means_[k])\n",
        "    print('cov = ')\n",
        "    print(gmm.covariances_[k])\n",
        "    print()"
      ],
      "metadata": {
        "id": "LGki5vGQeAaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## データに正規分布を重ねて表示\n",
        "\n",
        "# グラフ描画用のグリッドデータの作成\n",
        "x_mesh, y_mesh = np.mgrid[xmin:xmax:(xmax-xmin)/100, ymin:ymax:(ymax-ymin)/100]\n",
        "X_mesh = np.dstack((x_mesh, y_mesh))\n",
        "\n",
        "# グラフ\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(XOFG[:, 0], XOFG[:, 1])\n",
        "\n",
        "# Gaussian を当てはめた結果\n",
        "for k in range(K):\n",
        "    mu = gmm.means_[k]\n",
        "    cov = gmm.covariances_[k]\n",
        "    ax.contour(x_mesh, y_mesh, multivariate_normal.pdf(X_mesh, mean=mu, cov=cov))\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)"
      ],
      "metadata": {
        "id": "dUwPwgQ9fMD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "データの生成過程をモデル化しているので，疑似乱数を使ってこの GMM が表す分布に従うサンプルを無限に作り出すことができる．"
      ],
      "metadata": {
        "id": "cJ5jHmtG1I9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データを生成\n",
        "N = 200\n",
        "z = rng.choice(K, size=N, p=gmm.weights_)\n",
        "print(z)\n",
        "\n",
        "Xgen = np.empty((N, 2))\n",
        "for k in range(K):\n",
        "    Nk = np.sum(z == k)\n",
        "    mu = gmm.means_[k]\n",
        "    cov = gmm.covariances_[k]\n",
        "    Xgen[z == k, :] = rng.multivariate_normal(mu, cov, size=Nk)\n",
        "\n",
        "# 最初の10個\n",
        "for n in range(10):\n",
        "    print(z[n], Xgen[n, :])"
      ],
      "metadata": {
        "id": "BC3Kcb6-gSuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# グラフを描く\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4.5))\n",
        "ax[0].scatter(XOFG[:, 0], XOFG[:, 1], label='observed data')\n",
        "ax[0].scatter(Xgen[:, 0], Xgen[:, 1], label='generated')\n",
        "ax[0].set_xlim(xmin, xmax)\n",
        "ax[0].set_ylim(ymin, ymax)\n",
        "ax[0].legend()\n",
        "ax[1].scatter(Xgen[z == 0, 0], Xgen[z == 0, 1], label='k = 0', color='g')\n",
        "ax[1].scatter(Xgen[z == 1, 0], Xgen[z == 1, 1], label='k = 1', color='m')\n",
        "ax[1].set_xlim(xmin, xmax)\n",
        "ax[1].set_ylim(ymin, ymax)\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cCK089jwiDl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "左は，GMMを使って生成したデータを元のデータと重ねて描いたもの．\n",
        "右は，その生成されたデータがどちらの正規分布から生成されたのかを色分けして描いたもの．"
      ],
      "metadata": {
        "id": "WqAjJex-3D-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### EMアルゴリズムによる GMM のパラメータ推定"
      ],
      "metadata": {
        "id": "7jTJbmum3ZYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "データ $\\{ \\mathbf{x}_n \\in \\mathbb{R}^{D} \\}_{n=1}^{N}$ に GMM を当てはめる問題，すなわち，これらのデータに当てはまる GMM のパラメータを推定する問題を考える．\n",
        "\n",
        "以下，GMM のパラメータをまとめて $\\mathbf{\\theta}$ という記号で表すことにすると，対数尤度 $L(\\mathbf{\\theta})$ は次のようになる．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "L(\\mathbf{\\theta}) &= \\log \\left( \\prod_{n=1}^N p_{\\mathbf{\\theta}}(\\mathbf{x}_n) \\right) \\\\\n",
        "&= \\sum_{n=1}^N \\log{ p_{\\mathbf{\\theta}}(\\mathbf{x}_n) } \\\\\n",
        "&= \\sum_{n} \\log \\left( \\sum_{z_n} p_{\\mathbf{\\theta}}(\\mathbf{x}_n, z_n)  \\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "単一の正規分布の場合のようにこの対数尤度の最大化（最尤推定）を考えたいが，実はこの式のように $\\log$ の中に $\\sum$ が入った形をしているものは解析的に解くのが難しい．"
      ],
      "metadata": {
        "id": "oX51EE_FDwDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 対数尤度 = ELBO + KL-divergence"
      ],
      "metadata": {
        "id": "xowzvccX4pUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "対数尤度 $L(\\mathbf{\\theta})$ の最大化を実現する方法を探るために，式を変形してみる．\n",
        "ここでは，簡単のため，ひとつのデータの対数尤度 $\\log p_{\\mathbf{\\theta}}(\\mathbf{x})$ について考える（添字 $n$ も省略）．\n",
        "\n",
        "任意の確率分布 $q(z)$ に対して，$\\log p_{\\mathbf{\\theta}}(\\mathbf{x})$ は次のように式変形できる．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log{ p_{\\mathbf{\\theta}}(\\mathbf{x}) } &= \\log{ p_{\\mathbf{\\theta}}(\\mathbf{x}) } \\sum_z q(z) \\qquad  \\because \\sum_z q(z) = 1 \\\\\n",
        "&= \\sum_z q(z) \\log p_{\\mathbf{\\theta}}(\\mathbf{x}) \\\\\n",
        "&= \\sum_z q(z) \\log \\frac{p_{\\mathbf{\\theta}}(\\mathbf{x}, z)}{p_{\\mathbf{\\theta}}(z|\\mathbf{x})} \\qquad  \\because p_{\\mathbf{\\theta}}(\\mathbf{x}, z) = p_{\\mathbf{\\theta}}(z|\\mathbf{x})p_{\\mathbf{\\theta}}(\\mathbf{x}) \\\\\n",
        "&= \\sum_z q(z) \\log \\left( \\frac{p_{\\mathbf{\\theta}}(\\mathbf{x}, z)}{p_{\\mathbf{\\theta}}(z|\\mathbf{x})} \\cdot \\frac{q(z)}{q(z)} \\right) \\\\\n",
        "&= \\underbrace{\\sum_z q(z) \\log \\frac{p_{\\mathbf{\\theta}}(\\mathbf{x}, z)}{q(z)}}_{{\\rm ELBO}(\\mathbf{x}; \\mathbf{\\theta}, q)} + \\underbrace{\\sum_z q(z) \\log \\frac{q(z)}{p_{\\mathbf{\\theta}}(z|\\mathbf{x})}}_{D_{\\rm KL}(q(z) \\Vert p_{\\mathbf{\\theta}}(z|\\mathbf{x}))}\\\\\n",
        "&= {\\rm ELBO}(\\mathbf{x}; \\mathbf{\\theta}, q) + D_{\\rm KL}(q(z) \\Vert p_{\\mathbf{\\theta}}(z|\\mathbf{x}))\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "ここで，最後の式の第2項は，「$q(z)$ の $p_{\\mathbf{\\theta}}(z|\\mathbf{x})$ に対するKLダイバージェンス」と呼ばれる量で，$0$ 以上の実数値をとる．\n",
        "そのため，\n",
        "$\\log p_{\\mathbf{\\theta}}(\\mathbf{x}) \\geq {\\rm ELBO}(\\mathbf{x}; \\mathbf{\\theta}, q)$\n",
        "が成り立つ．\n",
        "すなわち，${\\rm ELBO}(\\mathbf{x}; \\mathbf{\\theta}, q)$ は $\\log p_{\\mathbf{\\theta}}(\\mathbf{x})$ の下界である（注）．\n",
        "したがって，$q(z)$を適当に定めたうえで ${\\rm ELBO}(\\mathbf{x}; \\mathbf{\\theta}, q)$ を大きくすれば，$\\log p_{\\mathbf{\\theta}}(\\mathbf{x})$ も大きくなる．\n",
        "また，その式は $\\log p_{\\mathbf{\\theta}}(\\mathbf{x})$ よりも扱いやすい形をしている．\n",
        "\n",
        "※ ELBO: Evidence Lower BOund, エビデンスの下界\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tNbN7a0bPGeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KL-divergence"
      ],
      "metadata": {
        "id": "1u6pUMuL6TrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KLダイバージェンス** （KL情報量， Kullback-Leibler divergence）は，2つの確率分布の「遠さ」を表す指標．\n",
        "\n",
        "\n",
        "連続確率分布 $p(x), q(x)$ において，$p(x)$ の $q(x)$ に対する KL ダイバージェンスは\n",
        "\n",
        "$$\n",
        "D_{\\rm KL}(p \\Vert q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx\n",
        "$$\n",
        "\n",
        "と定義される．また，離散確率分布 $p(x), q(x)$ において，$p(x)$ の $q(x)$ に対する KL ダイバージェンスは\n",
        "\n",
        "$$\n",
        "D_{\\rm KL}(p \\Vert q) = \\sum_{x} p(x) \\log \\frac{p(x)}{q(x)}\n",
        "$$\n",
        "\n",
        "と定義される．\n",
        "\n",
        "次の性質がある．\n",
        "\n",
        "- $D_{\\rm KL}(p \\Vert q) \\geq 0$．等号が成り立つのは2つの分布が一致する場合かつその場合に限られる\n",
        "- 一般に $D_{\\rm KL}(p \\Vert q) \\ne D_{\\rm KL}(q \\Vert p)$ となる．\n",
        "\n",
        "直感的には，2つの確率分布の遠さを表す，距離のような量と見なすことができるが，上記の2つ目のことから分かるように，距離としての性質は満たさない（対称性を満たさない）ことに注意が必要である．"
      ],
      "metadata": {
        "id": "4i0d9jVaWOrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EM アルゴリズム"
      ],
      "metadata": {
        "id": "rNU8sHk-6cY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上述の式変形の結果を利用して対数尤度 $\\log p_{\\mathbf{\\theta}}(\\mathbf{x})$ を最大化するためには，元々のパラメータ $\\mathbf{\\theta}$ に加えて $q(z)$ もパラメータとして最適化する必要がある．\n",
        "しかし，この最適化は，「$\\mathbf{\\theta}$を固定して $q(z)$ を更新する」手続き（「E-step」と呼ばれる）と，「$q(z)$を固定して$\\mathbf{\\theta}$を更新する」手続き（M-step）を交互に繰り返すことで実現できることが知られている．\n",
        "\n",
        "- E-step では，$\\mathbf{\\theta}$ が固定されているので，$q(z)$ をどのように選んでも $\\log p_{\\mathbf{\\theta}}(\\mathbf{x})$ は変化しない．このとき， 新しい $q(z)$ を $q(z) = p_{\\mathbf{\\theta}}(z|\\mathbf{x})$ ととれば， $D_{\\rm KL}(q(z) \\Vert p_{\\mathbf{\\theta}}(z|\\mathbf{x})) = 0 $ となって ${\\rm ELBO}(\\mathbf{x}; \\mathbf{\\theta}, q)$ が最大となる．\n",
        "- M-step では，$q(z)$ を上記で求めた値に固定して，${\\rm ELBO}(\\mathbf{x}; \\mathbf{\\theta}, q)$ を最大にする $\\mathbf{\\theta}$ を求める．\n",
        "\n",
        "\n",
        "E-step と M-step を交互に繰り返して対数尤度を最大化するこの最適化のアルゴリズムを，**EMアルゴリズム** (Expectation-Maximization Algorithm) という．\n"
      ],
      "metadata": {
        "id": "62MDUtfbcDhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここまでの説明では，一つのデータに対する対数尤度 $\\log p_{\\mathbf{\\theta}}(\\mathbf{x})$ の最大化を考えていた．しかし，実際には，複数のデータに対する対数尤度\n",
        "$L(\\mathbf{\\theta}) =\\sum_{n=1}^N \\log p_{\\mathbf{\\theta}}(\\mathbf{x}_n)$\n",
        "を最大化したい．\n",
        "この場合，EMアルゴリズムの手続きは次のようになる（導出は省略）．\n",
        "\n",
        "1. モデルパラメータ $\\mathbf{\\theta}$ を初期化する．\n",
        "1. E-step: 現在の $\\mathbf{\\theta}$ を用いて $q_1, q_2, \\ldots, q_N$ を更新する．\n",
        "1. M-step: 次の式を最大にする $\\mathbf{\\theta}$ を求める．\n",
        "$$\n",
        "\\sum_{n=1}^N {\\rm ELBO}(\\mathbf{x}_n; \\mathbf{\\theta}, q_n)\n",
        "$$\n",
        "1.　 繰り返しの終了条件を満たしていなければ 2. へ戻る\n",
        "\n",
        "\n",
        "終了条件は，「$L(\\mathbf{\\theta})$ の変化量が既定値以下になった」や「繰り返しが既定の回数に達した」等とするのが一般的．\n",
        "\n",
        "EMアルゴリズムでは，パラメータの更新を繰り返すごとに対数尤度 $L(\\mathbf{\\theta})$ が単調に増加する．すなわち，\n",
        "$t$ 回目の E-step, M-step によって得られたパラメータを $\\mathbf{\\theta}_{t}$ とおき，この値を用いて次の E-step, M-step を実行して得られるパラメータを $\\mathbf{\\theta}_{t+1}$ とおくと，\n",
        "$L(\\mathbf{\\theta}_{t+1}) \\geq L(\\mathbf{\\theta}_{t})$ が成り立つ．\n",
        "\n"
      ],
      "metadata": {
        "id": "VE6yMdRSpT-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EMアルゴリズムの概要を説明してきたが，実は，ここまで具体的な GMM の式は出てきていない．\n",
        "ここまでの議論は，GMM に限らず，潜在変数を持つモデル一般に成り立つものである．\n"
      ],
      "metadata": {
        "id": "wlSebRhC8vPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルを GMM に限定してより具体的なアルゴリズムを導出する過程の説明は省略する．\n",
        "結論としては，次のような手続きとなる．\n",
        "\n",
        "E-step: 現在のパラメータ $\\mathbf{\\theta}$ つまり $w_k, \\mathbf{\\mu}_k, \\Sigma_k$ を用いて $q_n(z = k) = p_{\\mathbf{\\theta}}(z = k|\\mathbf{x})$ を求める．\n",
        "以下，簡単のため，$q_n(z = k)$ を $q_{n,k}$ と表記する．\n",
        "$q_{n, k}$ は，$\\mathbf{x}_n$ が $k$ 番目の正規分布に所属する確率（その正規分布から生成された確率）の推定値とみなせる．\n",
        "\n",
        "$$\n",
        "q_{n, k} = \\frac{w_k {\\cal N}(\\mathbf{x}_n; \\mathbf{\\mu}_k, \\Sigma_k)}{\\sum_{j=1}^K w_j {\\cal N}(\\mathbf{x}_n; \\mathbf{\\mu}_j, \\Sigma_j)}\n",
        "$$\n",
        "\n",
        "\n",
        "M-step: 求めた $q_{n, k}$ を用いて，$w_k, \\mathbf{\\mu}_k, \\Sigma_k$ を更新する．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "w_k^{\\rm new} &= \\frac{1}{N}\\sum_{n=1}^N q_{n, k}\\\\\n",
        "\\mathbf{\\mu}_k^{\\rm new} &= \\frac{\\sum_{n=1}^{N} q_{n, k} \\mathbf{x}_n}{\\sum_{n=1}^{N} q_{n, k}} \\\\\n",
        "\\Sigma_k^{\\rm new} &= \\frac{\\sum_{n=1}^{N} q_{n, k} (\\mathbf{x}_n - \\mathbf{\\mu}_k)(\\mathbf{x}_n - \\mathbf{\\mu}_k)^{\\top}}{\\sum_{n=1}^{N} q_{n, k}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "OE6LyQ0Qi6wT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 実験: 別の2次元データ"
      ],
      "metadata": {
        "id": "dcB3qwTr95NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データの入手\n",
        "df = pd.read_csv('https://www-tlab.math.ryukoku.ac.jp/~takataka/course/AdvML/2dim3class.csv')\n",
        "X = df.drop(columns='label').to_numpy()"
      ],
      "metadata": {
        "id": "XkMOe9J15AhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# グラフを描く\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:, 0], X[:, 1])\n",
        "xmin, xmax = -5, 5\n",
        "ymin, ymax = -5, 5\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5w9hKClh5HMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 3\n",
        "gmm = GaussianMixture(n_components=K, covariance_type='full')\n",
        "gmm.fit(X)\n",
        "\n",
        "for k in range(K):\n",
        "    print(f'### {k}-th component')\n",
        "    print('weight = ', gmm.weights_[k])\n",
        "    print('mu = ', gmm.means_[k])\n",
        "    print('cov = ')\n",
        "    print(gmm.covariances_[k])\n",
        "    print()"
      ],
      "metadata": {
        "id": "wy9Dm5MB-KGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# グラフ描画用のグリッドデータの作成\n",
        "x_mesh, y_mesh = np.mgrid[xmin:xmax:(xmax-xmin)/100, ymin:ymax:(ymax-ymin)/100]\n",
        "X_mesh = np.dstack((x_mesh, y_mesh))\n",
        "\n",
        "# グラフ\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:, 0], X[:, 1])\n",
        "\n",
        "# Gaussian を当てはめた結果\n",
        "for k in range(K):\n",
        "    mu = gmm.means_[k]\n",
        "    cov = gmm.covariances_[k]\n",
        "    ax.contour(x_mesh, y_mesh, multivariate_normal.pdf(X_mesh, mean=mu, cov=cov))\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')"
      ],
      "metadata": {
        "id": "JvhDgbP36FUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) を読んで，`K` や `covariance_type` を変えて実験してみよう．"
      ],
      "metadata": {
        "id": "u2OttNWF-qA6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}