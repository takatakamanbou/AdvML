{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/AdvML/blob/2024/AdvML2024_ex06notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# AdvML ex06notebookA\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/AdvML/AdvML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?AdvML)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "板書や口頭で補足する前提なので，この notebook だけでは説明が不完全です．\n",
        "\n",
        "今回の話は，学部の科目「機械学習I」でも出てきています（復習しつつ一歩先へ進む感じ）．受講していないひとは，以下をどうぞ．\n",
        "\n",
        "- 2024年度「機械学習I」 第5回 https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2024#ex05\n",
        "- 2024年度「機械学習I」 第6回 https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2024#ex06\n",
        "\n"
      ],
      "metadata": {
        "id": "Kh4ZRfEvArX9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ0oDxG3iygx"
      },
      "source": [
        "----\n",
        "## 準備\n",
        "----\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc  # アニメーションのため\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "# scikit-learn のいろいろ\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.neural_network import MLPRegressor, MLPClassifier"
      ],
      "metadata": {
        "id": "RjLTO96x2EvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2次元2クラス識別のサンプルデータ"
      ],
      "metadata": {
        "id": "1pGgU4anaoJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moonX, moonY = make_moons(n_samples=200, noise=0.25)\n",
        "\n",
        "# グラフを描く\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "ax.scatter(moonX[moonY==0, 0], moonX[moonY==0, 1])\n",
        "ax.scatter(moonX[moonY==1, 0], moonX[moonY==1, 1])\n",
        "ax.set_xlim(-1.5, 2.5)\n",
        "ax.set_ylim(-2, 2)\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UWFJkjgDaozN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 階層型ニューラルネットワーク\n",
        "---\n"
      ],
      "metadata": {
        "id": "qrKE45oj_dUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 線形モデルの限界"
      ],
      "metadata": {
        "id": "JSqZjrEtZnM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 線形回帰"
      ],
      "metadata": {
        "id": "AfmPTFT8Zr4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$D$次元入力の線形回帰モデル\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = w_0 + w_1x_1 + w_2x_2 + \\cdots + w_Dx_D\n",
        "$$\n",
        "\n",
        "は，$\\mathbf{x}$ と出力の正解が作る $(D+1)$ 次元空間の中で $D$次元の平面（超平面）を表す．\n",
        "そのため，平面にうまく当てはまらないようなデータではよい予測ができない．\n",
        "\n",
        "<img src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/planefitting2.png\" width=\"75%\">"
      ],
      "metadata": {
        "id": "LoS7fE7yd9Oq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ロジスティック回帰"
      ],
      "metadata": {
        "id": "2jn7_FYGZuC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$D$ 次元入力を 2 クラスに分類するロジスティック回帰モデル\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f(\\mathbf{x}) &= \\sigma(w_0 + w_1x_1+\\cdots + w_Dx_D) = \\sigma\\left(w_0 + \\sum_{d=1}^{D}w_dx_d \\right) \\\\\n",
        "&= \\frac{1}{1+\\exp{\\left( - \\left( w_0 + \\sum_{d=1}^{D}w_dx_d \\right) \\right)}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "において，$f(\\mathbf{x}) = \\frac{1}{2} \\Leftrightarrow w_0 + \\sum_{d=1}^{D} w_dx_d = 0$ だから，2クラスの識別境界は平面（$D$次元空間中の $(D-1)$次元超平面，$D=2$のときは直線）となる．\n",
        "多クラスの場合も同様．そのため，上記の例のような場合にはうまく識別できない．"
      ],
      "metadata": {
        "id": "Qa1_DE_ScEBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記の2次元2クラス識別の問題をロジスティック回帰で解いてみると..."
      ],
      "metadata": {
        "id": "tz2WLmoxbcTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ロジスティック回帰\n",
        "model = LogisticRegression()\n",
        "model.fit(moonX, moonY)\n",
        "yy = model.predict(moonX)\n",
        "cc = np.sum(yy == moonY)\n",
        "print(f'正答率: {cc}/{len(yy)} = {cc/len(yy):.3f}')"
      ],
      "metadata": {
        "id": "116gh4CIbtwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "識別境界を可視化してみると..."
      ],
      "metadata": {
        "id": "_0v7JQ8Yb8rW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# グラフ描画用のグリッドデータの作成\n",
        "K = 2\n",
        "xmin, xmax = -1.5, 2.5\n",
        "ymin, ymax = -2, 2\n",
        "npoints = 100\n",
        "dx, dy = (xmax - xmin)/npoints, (ymax - ymin)/npoints\n",
        "x_mesh, y_mesh = np.mgrid[xmin:xmax:dx, ymin:ymax:dy]\n",
        "X_mesh = np.dstack((x_mesh, y_mesh))\n",
        "\n",
        "# X_mesh の各点における事後確率の推定\n",
        "p = model.predict_proba(X_mesh.reshape((-1, 2)))\n",
        "pp = p.reshape((X_mesh.shape[0], X_mesh.shape[1], K))\n",
        "\n",
        "# グラフの描画\n",
        "fig, ax = plt.subplots()\n",
        "cmap = ['Blues', 'Oranges', 'Greens']\n",
        "for k in range(K):\n",
        "    ax.scatter(moonX[moonY==k, 0], moonX[moonY==k, 1])\n",
        "    ax.contourf(x_mesh, y_mesh, pp[:, :, k], levels=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0], cmap=cmap[k], alpha=0.3)\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A5biloWqb4ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### 階層型ニューラルネットワーク"
      ],
      "metadata": {
        "id": "npFWkSluZySK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 例: 2層ニューラルネットによる非線形回帰"
      ],
      "metadata": {
        "id": "qnz-Jd_2Z1ND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### モデルの構造と活性化関数"
      ],
      "metadata": {
        "id": "ci0uFlbPn-Ui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$D$ 次元入力 $\\mathbf{x}$ から $y$ の値を予測する回帰モデル $f(\\mathbf{x})$ を，次式のように定義してみる．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f(x) = z^{\\rm O} &= \\sum_{h=1}^{H} w_h^{\\rm O} z_h^{\\rm H} \\\\\n",
        "z_h^{\\rm H} &= \\sigma\\left( b_{h}^{\\rm H} + \\sum_{d=1}^{D}w_{h, d}^{\\rm H} x_d  \\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "$\\sigma(s)$ は **活性化関数** (activation function) と呼ばれるものである（どのようなものかは後述）．\n",
        "このモデルのパラメータは，$w_h^{\\rm O}, b_h^{\\rm H}$ および $w_{h, d}^{H}$ ($h = 1, 2, \\ldots, H, d = 1, 2, \\ldots , D$) である．\n",
        "このモデルで適当な入力 $\\mathbf{x}$ に対する出力 $z^{\\rm O}$ を計算する場合，$\\mathbf{x}$ を使って $z_h^{\\rm H}$ を求めるステップと，$z_h^{\\rm H}$ を使って $z^{\\rm O}$ の値を求めるステップの2段階に分けることができる → 階層構造，ニューロン．\n",
        "このモデルの場合は隠れ層 (hidden layer) と出力層の2層から成る．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8lf_fkEoijhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "隠れ層のニューロンが非線形の活性化関数をもつことで，$f(x)$ は非線形関数を表すことが可能となる．ニューラルネットでよく用いられるのは，ロジスティック回帰モデルでも使われている**シグモイド関数**(sigmoid function)の他，**双曲線正接関数**(hyperbolic tangent function)，**Rectified Linear 関数**（**ReLU関数**ともいいます）など（注）．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mbox{Logistic Sigmoid}\\quad  & \\sigma(s) = \\frac{1}{1+{\\rm e}^{-s}}\\\\\n",
        "\\mbox{Hyperbolic Tangent}\\quad & \\sigma(s) = {\\rm tanh}(s) = \\frac{{\\rm e}^{s} - {\\rm e}^{-s}}{{\\rm e}^{s} + {\\rm e}^{-s}}\\\\\n",
        "\\mbox{Rectified Linear}\\quad & \\sigma(s) = \\left\\{\n",
        "    \\begin{array}{ll}\n",
        "    s & (s \\ge 0)\\\\\n",
        "    0 & {\\rm otherwise}\\\\\n",
        "\\end{array} \\right.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 「シグモイド」は「S字状の」という意味の語なので，双曲線正接関数なども含めて類似した形をしている関数たちの総称です．式(X)のものは，正確には「ロジスティックシグモイド関数」(logistic sigmoid function)といいます．\n",
        "また，「Rectified Linear」な活性化関数を採用したニューロンを Rectified Linear Unit と呼ぶことから，この活性化関数を「ReLU」と呼ぶことがあります．\n",
        "</span>\n"
      ],
      "metadata": {
        "id": "CBxL7CuCm0vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 活性化関数の値を計算\n",
        "xmin, xmax = -4, 4\n",
        "X = np.linspace(xmin, xmax, num=100)\n",
        "Y1 = 1/(1+np.exp(-X)) # ロジスティックシグモイド\n",
        "Y2 = np.tanh(X)       # 双曲線正接\n",
        "Y3 = np.maximum(X, 0) # ReLU\n",
        "\n",
        "# グラフに描く\n",
        "fig = plt.figure(figsize=(12,3))\n",
        "ax1 = fig.add_subplot(131)\n",
        "ax1.set_xlim(xmin, xmax)\n",
        "ax1.set_ylim(-1.2, 1.2)\n",
        "ax1.axhline(y=0, color='black', linestyle='-')\n",
        "ax1.axvline(x=0, color='black', linestyle='-')\n",
        "ax1.axhline(y=1, color='gray', linestyle='--')\n",
        "ax1.plot(X, Y1, c='red', linewidth=3, label='Sigmoid')\n",
        "ax1.legend()\n",
        "ax2 = fig.add_subplot(132)\n",
        "ax2.set_xlim(xmin, xmax)\n",
        "ax2.set_ylim(-1.2, 1.2)\n",
        "ax2.axhline(y=0, color='black', linestyle='-')\n",
        "ax2.axvline(x=0, color='black', linestyle='-')\n",
        "ax2.axhline(y=1, color='gray', linestyle='--')\n",
        "ax2.axhline(y=-1, color='gray', linestyle='--')\n",
        "ax2.plot(X, Y2, c='red', linewidth=3, label='tanh(s)')\n",
        "ax2.legend()\n",
        "ax3 = fig.add_subplot(133)\n",
        "ax3.set_xlim(xmin, xmax)\n",
        "ax3.set_ylim(xmin, xmax)\n",
        "ax3.axhline(y=0, color='black', linestyle='-')\n",
        "ax3.axvline(x=0, color='black', linestyle='-')\n",
        "ax3.plot(X, Y3, c='red', linewidth=3, label='ReLU')\n",
        "ax3.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xvJGp8Wlk6vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 学習"
      ],
      "metadata": {
        "id": "1mRiUFLDoG2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習データが $(\\mathbf{x}_n, y_n)$ ($n=1,2,\\ldots, N$) と与えられたときに，線形回帰と同様に，二乗誤差\n",
        "\n",
        "$$\n",
        "E = \\frac{1}{2}\\sum_{n=1}^{N} (y_n - f(\\mathbf{x}_n))^2\n",
        "$$\n",
        "\n",
        "が最小となるようなパラメータを求める．ただし，線形回帰と違って一撃の計算では最適なパラメータが求まらないので，勾配法を用いる．そのため，活性化関数は微分可能でなければならない．"
      ],
      "metadata": {
        "id": "xHCpGDvnoP_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "入力が1次元（$D=1$）の場合に，パラメータの勾配を求めてみよう．\n",
        "$e_n = \\frac{1}{2}(y_n - z_n^{\\rm O})^2$ とおく．\n",
        "活性化関数にロジスティックシグモイドを用いる場合，$\\frac{d \\sigma(s)}{ds} = \\sigma(s)(1-\\sigma(s))$ となる．\n",
        "これを用いて，\n",
        "$\\frac{\\partial e_n}{\\partial w_h^{\\rm O}}$, $\\frac{\\partial e_n}{\\partial w_h^{\\rm H}}$, $\\frac{\\partial e_n}{\\partial b_h^{\\rm H}}$ を求めてみよう．"
      ],
      "metadata": {
        "id": "FfFpsuaiqjiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットの出力の計算時には，ニューラルネットへ入力された信号は，入力に近い側の層から順に層の間を伝わって出力へと至る．\n",
        "これに対して，勾配の計算時には，出力層で計算された誤差の値が，入力側の層へと逆向きに伝わっていくとみなすことができる．\n",
        "そのため，勾配法によるニューラルネットの学習は，**誤差逆伝播学習** (error back-propagation learning)と呼ばれることもある．"
      ],
      "metadata": {
        "id": "4ksZxGcgTCDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 例: 2層ニューラルネットによる2クラス識別"
      ],
      "metadata": {
        "id": "6PYrOfqtZ67M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次式は，$D$次元入力を2クラスに分類する2層ニューラルネットモデルである．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f(\\mathbf{x}) = z^{\\rm O} &= \\sigma^{\\rm O} \\left( b^{\\rm O} + \\sum_{h=1}^{H} w_h^{\\rm O} z_h^{\\rm H} \\right) \\\\\n",
        "z_h^{\\rm H} &= \\sigma^{\\rm H}\\left( b_{h}^{\\rm H} + \\sum_{d=1}^{D}w_{h,d}^{\\rm H} x_d  \\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "隠れ層の活性化関数 $\\sigma^{\\rm H}$ には，ロジスティックシグモイドや ReLU 等を用いる．\n",
        "出力層の活性化関数 $\\sigma^{\\rm O}$ には，ロジスティック回帰と同様にロジスティックシグモイド関数を用いる．"
      ],
      "metadata": {
        "id": "NgJHY3pOr_YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力クラスの正解を付した学習データに対して，ロジスティック回帰と同様に，交差エントロピーを最小化するようなパラメータを勾配法によって求めることで，2クラス識別を学習させることができる．"
      ],
      "metadata": {
        "id": "2CFT_KMcT7Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title デモ: 学習させるモデルを選んでね\n",
        "option = 'logistic' #@param ['logistic', '2-layer (1000,)', '2-layer (2000,)', '3-layer (100, 100)', '3-layer (1000, 1000)'] {allow-input: false}\n",
        "\n",
        "if option == 'logistic':\n",
        "    model = LogisticRegression()\n",
        "else:\n",
        "    if option == '2-layer (1000,)':\n",
        "        numNeurons = (1000, )\n",
        "    elif option == '2-layer (2000,)':\n",
        "        numNeurons = (2000, )\n",
        "    elif option == '3-layer (100, 100)':\n",
        "        numNeurons = (100, 100, )\n",
        "    elif option == '3-layer (1000, 1000)':\n",
        "        numNeurons = (1000, 1000, )\n",
        "    model = MLPClassifier(hidden_layer_sizes=numNeurons, activation='relu', verbose=True)\n",
        "\n",
        "model.fit(moonX, moonY)\n",
        "yy = model.predict(moonX)\n",
        "cc = np.sum(yy == moonY)\n",
        "print()\n",
        "print(f'{option}   正答率: {cc}/{len(yy)} = {cc/len(yy):.3f}')"
      ],
      "metadata": {
        "id": "DSZpv1WlCT_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# グラフ描画用のグリッドデータの作成\n",
        "K = 2\n",
        "xmin, xmax = -1.5, 2.5\n",
        "ymin, ymax = -2, 2\n",
        "npoints = 100\n",
        "dx, dy = (xmax - xmin)/npoints, (ymax - ymin)/npoints\n",
        "x_mesh, y_mesh = np.mgrid[xmin:xmax:dx, ymin:ymax:dy]\n",
        "X_mesh = np.dstack((x_mesh, y_mesh))\n",
        "\n",
        "# X_mesh の各点における事後確率の推定\n",
        "p = model.predict_proba(X_mesh.reshape((-1, 2)))\n",
        "pp = p.reshape((X_mesh.shape[0], X_mesh.shape[1], K))\n",
        "\n",
        "# グラフの描画\n",
        "fig, ax = plt.subplots()\n",
        "cmap = ['Blues', 'Oranges', 'Greens']\n",
        "for k in range(K):\n",
        "    ax.scatter(moonX[moonY==k, 0], moonX[moonY==k, 1])\n",
        "    ax.contourf(x_mesh, y_mesh, pp[:, :, k], levels=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0], cmap=cmap[k], alpha=0.3)\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIzwdnqmFd9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 階層型ニューラルネットワークと深層学習"
      ],
      "metadata": {
        "id": "zURxffqPS5kB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力ニューロンは複数にすることもできる．例えば，$K$ 個出力ニューロンを持つニューラルネットで，出力層の活性化関数を softmax として交差エントロピーを最小化するように学習させれば，$K$ クラスの識別ができる．\n",
        "また，隠れ層を複数用いることもできる．\n",
        "\n",
        "隠れ層の数や層ごとのニューロンの数，活性化関数の選択，etc. はハイパーパラメータ\n",
        "\n"
      ],
      "metadata": {
        "id": "ATPgFlxeUwnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**深層学習** (deep learning)という語の意味は..."
      ],
      "metadata": {
        "id": "ZrX5yxKHV-rC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84shp5meS8rF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}