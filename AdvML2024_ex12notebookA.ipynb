{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/AdvML/blob/2024/AdvML2024_ex12notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# AdvML ex12notebookA\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/AdvML/AdvML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?AdvML)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "板書や口頭で補足する前提なので，この notebook だけでは説明が不完全です．\n"
      ],
      "metadata": {
        "id": "bStHGoTvH0gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 教師なし学習，クラスタリング，次元削減\n",
        "---\n"
      ],
      "metadata": {
        "id": "qrKE45oj_dUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 準備\n"
      ],
      "metadata": {
        "id": "zl03icnhjxTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.mixture import GaussianMixture"
      ],
      "metadata": {
        "id": "CY8ptRIEj3eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 教師なし学習とは\n"
      ],
      "metadata": {
        "id": "fZtSxUWk88qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**教師あり学習** (supervised learning): 個々の学習データが，「入力」とそれに対する「出力の正解」のペアとして与えられる．\n",
        "\n",
        "**教師なし学習** (unsupervised learning): 学習データは入力のみで構成され，出力の正解は与えられない．\n",
        "\n",
        "教師なし学習の目的は，大量のデータが与えられたときに，そのデータのもつ規則性や構造を見つけ出すこと．それらを知り，データから有用な情報を抽出することを目指す．\n",
        "教師あり学習では出力の正解が学習データとして与えられたが，\n",
        "教師なし学習では，学習データは入力となるもののみが与えられ，出力の正解は与えられない．\n",
        "データからどのような規則性や構造を見出したいかによって様々な問題設定や手法があり，出力がどのようなものになるかはその選択による．\n"
      ],
      "metadata": {
        "id": "c4PTuEd7hWPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "教師なし学習の問題の代表例\n",
        "\n",
        "- **クラスタリング**(clustering)\n",
        "- **次元削減**(次元圧縮とも，dimensionality reduction)\n",
        "\n",
        "正規分布や GMM の当てはめによる **確率密度推定** (probability density estimation) も，教師なし学習の問題の一種とみなせる．"
      ],
      "metadata": {
        "id": "3LcaKsdKhmP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### クラスタリング"
      ],
      "metadata": {
        "id": "4ungsyBz9I_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**クラスタリング** (clustering) は，大量のデータをいくつかの塊（**クラスタ** (cluster)）に分ける手続き．\n",
        "クラスタリングの手法は，**階層型クラスタリング** と **非階層型クラスタリング** に大別できる．\n",
        "\n",
        "**階層型クラスタリング** (hierarchical clustering): 「クラスタAとクラスタBをあわせたものがクラスタPで，クラスタPとクラスタQをあわせたものがクラスタR」というように，階層的になったクラスタを作るクラスタリング手法．\n",
        "\n",
        "**非階層型クラスタリング** (non-hierarchical clustering): クラスタ同士に上記のような階層構造をつくらないクラスタリング手法．\n",
        "\n",
        "それぞれ，実際の手法には様々なものがあり，データや問題の性質に応じて使い分けられる．\n",
        "この授業では，階層型クラスタリングについては説明しない（学部2年次科目「[多変量解析及び演習](https://www-tlab.math.ryukoku.ac.jp/wiki/?MVA)」参照．[MVA2023では第12回](https://www-tlab.math.ryukoku.ac.jp/wiki/?MVA/2023#ex12)）．\n",
        "\n"
      ],
      "metadata": {
        "id": "WFbvPbBYg61h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-means アルゴリズム"
      ],
      "metadata": {
        "id": "i6mKSAwp9L9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$K$-means アルゴリズム**（K-平均法， K-means method）は，データ同士の間のユークリッド距離が計算できるデータを対象としたクラスタリングのアルゴリズム．\n",
        "データをいくつのクラスタに分けるかを予め決めておいて，各データをどれか一つのクラスタに割り振る．\n",
        "名前の$K$は予め定めるクラスタの数を表す．\n",
        "\n",
        "例えば，下図左のような2次元のデータに対して，クラスタ数を $K=3$ として$K$-meansアルゴリズムを適用すると，下図右のような結果が得られる．\n",
        "\n",
        "<img src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/kmeans.png\">\n",
        "\n",
        "右の図の各データ点は，3つのクラスタのうちのどれに割り振られたかに応じて3色に塗り分けられている．\n",
        "図中に描かれた3つの★印は，それぞれのクラスタに所属するデータの重心を表している．\n",
        "$K$-meansアルゴリズムでは，クラスタごとのデータの重心のことを **セントロイド**(centroid)と呼ぶ．"
      ],
      "metadata": {
        "id": "xhto5VvyI0oG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "［**K-means アルゴリズム**］\n",
        "\n",
        "$N$個の$D$次元ベクトルから成るデータ集合\n",
        "$$\n",
        "\\{ \\mathbf{x}_n \\in {\\cal R}^{D} | n = 1, 2, \\ldots, N\\}\n",
        "$$\n",
        "を学習データとして，これを$K$個のクラスタ $C_1, C_2, \\ldots, C_K$ に分ける $K$-means アルゴリズムは，次のようになる．\n",
        "\n",
        "(0) クラスタごとのセントロイド $\\mathbf{c}_{k} \\in {\\cal R}^{D}$ ($k=1, 2, \\ldots, K$)の初期値を適当に決める．\n",
        "\n",
        "(1) 学習データ $\\mathbf{x}_n$ ($n = 1, 2, \\ldots, N$) のそれぞれを，次の手順で $C_1, C_2, \\ldots, C_K$ のいずれかに割り振る．\n",
        "- $\\mathbf{x}_n$ に対して，$K$個のセントロイドのうち最も距離の小さいものの番号 $y_n$ を次式のように求める．\n",
        "$$\n",
        "\\newcommand{\\argmin}{\\mathop{\\rm argmin}\\limits}\n",
        "y_n = \\argmin_{k=1,2,\\ldots,K}\\Vert \\mathbf{x}_{n} - \\mathbf{c}_{k} \\Vert^2\n",
        "$$\n",
        "- $\\mathbf{x}_n$ をクラスタ $C_{y_n}$ に割り振る\n",
        "\n",
        "(2) 各クラスタに割り振られた学習データたちの重心を求め，その値でそれぞれのセントロイドを更新する．式で書くと次の通り（注）．\n",
        "$$\n",
        "\\mathbf{c}_{k} = \\frac{1}{|C_k|}\\sum_{n:y_n = k} \\mathbf{x}_n\n",
        "$$\n",
        "\n",
        "(3) 結果が一定の条件を満たしていれば終了，さもなくば (1) へ戻る．\n",
        "\n",
        "\n",
        "※ 注: この式の和は，$y_n$ が $k$ と等しいような $n$ たちについてとる．すなわち，クラスタ $C_k$ に所属するデータの和を意味する．また，$|C_k|$ は集合 $C_k$ の元の数，つまり，クラスタ $C_k$ に割り振られたデータの個数を表す．\n",
        "\n"
      ],
      "metadata": {
        "id": "6Dv5TTcd__zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$K$-平均法には次のような性質がある．\n",
        "\n",
        "性質1: $K$-平均法の学習では，次式で表される量が最小化される．\n",
        "$$\n",
        "E = \\sum_{k=1}^{K}\\sum_{n:y_n = k} \\Vert \\mathbf{x}_n - \\mathbf{c}_{k}\\Vert^2\n",
        "$$\n",
        "この値は，「学習データのそれぞれが割り振られたクラスタのセントロイドとの距離の二乗」の和である．アルゴリズムの繰り返しごとに，この $E$ の値は単調減少する．\n",
        "\n",
        "性質2: $K$-meansアルゴリズムの学習結果は，初期値のとり方によって変わる．初期値によっては，$E$ の（最小でない）極小解に到達してそれ以上解が変化しなくなる．そのため，実用の際は，何通りかの初期値で学習を繰り返し，$E$の値が最も小さかった結果を採用する，といった方法がとられる．"
      ],
      "metadata": {
        "id": "p8ONk5anK6gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 実験: 2次元データに K-means アルゴリズムを適用してみる"
      ],
      "metadata": {
        "id": "PEZj-ub09sFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 準備\n",
        "\n",
        "データと K-means アルゴリズムの学習手続きの関数の定義．"
      ],
      "metadata": {
        "id": "OcB02EHr92Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 実験用データの入手\n",
        "df = pd.read_csv('https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/data4kmeans.csv')\n",
        "dat1 = df[['x1', 'x2']].to_numpy()\n",
        "dat2 = df[['y1', 'y2']].to_numpy()"
      ],
      "metadata": {
        "id": "waRFohAwrOn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## セントロイドの初期化\n",
        "#\n",
        "def initCentroid(X, centroid, seed=None):\n",
        "    assert X.shape[1] == centroid.shape[1]\n",
        "    K = centroid.shape[0]\n",
        "    # 学習データからランダムに K 個を選択して初期セントロイドとする\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N, dtype=int)\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    np.random.shuffle(idx)\n",
        "    centroid[:] = X[idx[:K], :]\n",
        "\n",
        "## データをクラスタに割り振る\n",
        "#\n",
        "def assignCluster(X, centroid, label):\n",
        "    assert X.shape[1] == centroid.shape[1] and X.shape[0] == label.shape[0]\n",
        "    K = centroid.shape[0]\n",
        "    N = X.shape[0]\n",
        "    sqe = 0.0\n",
        "    for n in range(N):\n",
        "        # 各セントロイドとの距離の二乗を計算\n",
        "        d = np.sum((X[n, :] - centroid)**2, axis=1)\n",
        "        # 距離最小のクラスタへ割り振る\n",
        "        i = np.argmin(d)\n",
        "        label[n] = i\n",
        "        sqe += d[i]\n",
        "\n",
        "    return sqe/N  # 割り振られたセントロイドとの距離の二乗の平均\n",
        "\n",
        "## セントロイドを計算し直す\n",
        "#\n",
        "def updateCentroid(X, centroid, label):\n",
        "    assert X.shape[1] == centroid.shape[1] and X.shape[0] == label.shape[0]\n",
        "    K = centroid.shape[0]\n",
        "    for ik in range(K):\n",
        "        # ik 番目のクラスタに割り当てられたデータの平均をそのクラスタの新しいセントロイドとする\n",
        "        centroid[ik, :] = np.mean(X[label==ik, :], axis=0)"
      ],
      "metadata": {
        "id": "b-rNgijPNVPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### データその1"
      ],
      "metadata": {
        "id": "rSmX47HN-E10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のコードセルをそのまま実行すると，`dat1` という変数に格納された2次元データの散布図を描く．"
      ],
      "metadata": {
        "id": "Qv1dlpCh-Too"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = dat1  #　データその1\n",
        "#X = dat2  # データその2\n",
        "\n",
        "# クラスタリング結果描画用のデータ\n",
        "xmin, xmax = -5, 5\n",
        "ymin, ymax = -5, 5\n",
        "p = np.dstack(np.mgrid[xmin:xmax:0.05, ymin:ymax:0.05])\n",
        "P = p.reshape((-1, p.shape[2]))\n",
        "\n",
        "# 散布図\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(4, 4))\n",
        "ax.scatter(X[:, 0], X[:, 1])\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uDZEK7lzMCbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のコードセルは，K-means アルゴリズムの初期化の手続きである．"
      ],
      "metadata": {
        "id": "YMQpFZ63-agN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 初期化\n",
        "\n",
        "K = 3  # クラスタ数\n",
        "\n",
        "N, D = X.shape\n",
        "centroid = np.empty((K, D)) # セントロイド\n",
        "label = np.empty(N, dtype=int) # 各学習データの所属するセントロイドの番号\n",
        "labelP = np.empty(P.shape[0], dtype=int)\n",
        "\n",
        "initCentroid(X, centroid)  # セントロイドを初期化\n",
        "\n",
        "i = 0  # K-平均法の繰り返し回数"
      ],
      "metadata": {
        "id": "Jpz6iYuoOBWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上のセルを実行して初期化を行ったのち，下のセルを実行すると，K-meansアルゴリズムの繰り返しの1回分を実行することができる．\n",
        "繰り返し実行して，学習が進む様子を観察しよう．"
      ],
      "metadata": {
        "id": "GlMlI7EJ-fIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1ステップの学習を実行\n",
        "\n",
        "# データを各クラスタに割り振る\n",
        "msqe = assignCluster(X, centroid, label)\n",
        "assignCluster(P, centroid, labelP)\n",
        "\n",
        "# 現在のクラスタ割り振り結果を描画\n",
        "colors = seaborn.color_palette(n_colors=K)\n",
        "fig = plt.figure(facecolor=\"white\", figsize=(10, 5))\n",
        "ax0 = fig.add_subplot(121)\n",
        "ax0.set_xlim(xmin, xmax)\n",
        "ax0.set_ylim(ymin, ymax)\n",
        "ax0.set_aspect('equal')\n",
        "for ik in range(K):\n",
        "    Xk = X[label==ik, :]\n",
        "    Pk = P[labelP==ik, :]\n",
        "    ax0.scatter(Xk[:, 0], Xk[:, 1], color=colors[ik])\n",
        "    ax0.plot(centroid[ik, 0], centroid[ik, 1], color='white', marker='*', markerfacecolor=colors[ik], markersize=25)\n",
        "    ax0.scatter(Pk[:, 0], Pk[:, 1], marker='.', alpha=0.1, color=colors[ik])\n",
        "    ax0.text(xmin+0.5, ymin+0.5, f'step{i}: {msqe:.3f}', size=20)\n",
        "\n",
        "# セントロイドを更新\n",
        "updateCentroid(X, centroid, label)\n",
        "i += 1\n",
        "\n",
        "# 新しいセントロイドを表示\n",
        "ax1 = fig.add_subplot(122)\n",
        "ax1.set_xlim(xmin, xmax)\n",
        "ax1.set_ylim(ymin, ymax)\n",
        "ax1.set_aspect('equal')\n",
        "for ik in range(K):\n",
        "    Xk = X[label==ik, :]\n",
        "    ax1.scatter(Xk[:, 0], Xk[:, 1], color=colors[ik])\n",
        "    ax1.plot(centroid[ik, 0], centroid[ik, 1], color='white', marker='*', markerfacecolor=colors[ik], markersize=25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a6Kn9YmbO-qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means アルゴリズムは，パラメータの初期値（初期セントロイドの選び方）によって学習結果が変わる．初期化からやり直してみよう．\n",
        "\n",
        "さらに，K の値を変えて実験してみよう．"
      ],
      "metadata": {
        "id": "OAdgds8o_Fjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### データその2\n"
      ],
      "metadata": {
        "id": "uoQ03Wt__efD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "「データその1」の最初のコードセルの最初の2行のコメントを付け替えることで，`dat2` という変数に入ったデータでも実験することができる．"
      ],
      "metadata": {
        "id": "Tc-pht_8_i_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### GMM を用いたクラスタリング"
      ],
      "metadata": {
        "id": "xNPTGNir_w9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "混合正規分布モデル(GMM)を用いてクラスタリングすることもできる．\n",
        "GMM は\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(\\mathbf{x}) = \\sum_{k=1}^{K} w_k {\\cal N}(\\mathbf{x}; \\mathbf{\\mu}_k, \\Sigma_k)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "という式で与えられる．データが $K$ 個の正規分布のうちどれから生成されたかを表す潜在変数を $z$ とおくと，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(z = k) &= w_k\\\\\n",
        "p(\\mathbf{x}|z = k) &= {\\cal N}(\\mathbf{x}; \\mathbf{\\mu}_k, \\Sigma_k)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "である．よって，\n",
        "\n",
        "$$\n",
        "p(\\mathbf{x}, z = k) = p(\\mathbf{x}|z = k) p(z = k) = p(z = k|\\mathbf{x}) p(\\mathbf{x})\n",
        "$$\n",
        "\n",
        "より，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(z = k|\\mathbf{x}) &= \\frac{p(\\mathbf{x}|z = k) p(z = k)}{p(\\mathbf{x})}\\\\\n",
        "&= \\frac{w_k {\\cal N}(\\mathbf{x}; \\mathbf{\\mu}_k, \\Sigma_k)}{\\sum_{j=1}^{K} w_j {\\cal N}(\\mathbf{x}; \\mathbf{\\mu}_j, \\Sigma_j)}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "が成り立つ．\n"
      ],
      "metadata": {
        "id": "eY8H-gK8Ajy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習データに GMM を当てはめたのちに，データ $\\mathbf{x}$ に対して $p(z = k|\\mathbf{x}) $ ($k = 1, 2, \\ldots, K$) を求めれば，$k$ 番目の正規分布が表すクラスタにこのデータが所属する確率を求めることができる．\n",
        "\n",
        "K-means アルゴリズムでは，個々のデータは $K$ 個のクラスタのうちのどれか一つに割り当てられた．GMM を用いる場合は，上記のように確率的にクラスタに割り当てることができる．前者のようなクラスタの割り当てかたを hard-assignment といい，後者の割り当て方を soft-assignment という（注）．\n",
        "\n",
        "※注: $\\underset{k}{\\operatorname{argmax}}p(z = k|\\mathbf{x})$ として hard-assignment 化することも可能．\n"
      ],
      "metadata": {
        "id": "F59TboEk_1Xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次の2つのコードセルを実行すると，↑と同じデータを GMM を用いてクラスタリングすることができる．"
      ],
      "metadata": {
        "id": "c_KpGqfXFTnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = dat1  #　データその1\n",
        "#X = dat2  # データその2\n",
        "\n",
        "# GMM の当てはめ\n",
        "K = 3\n",
        "gmm = GaussianMixture(n_components=3, covariance_type='full')\n",
        "gmm.fit(X)\n",
        "\n",
        "# グラフ描画用のグリッドデータの作成\n",
        "xmin, xmax = -5, 5\n",
        "ymin, ymax = -5, 5\n",
        "x_mesh, y_mesh = np.mgrid[xmin:xmax:(xmax-xmin)/100, ymin:ymax:(ymax-ymin)/100]\n",
        "X_mesh = np.dstack((x_mesh, y_mesh))\n",
        "\n",
        "# グラフ\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:, 0], X[:, 1])\n",
        "\n",
        "# Gaussian を当てはめた結果\n",
        "for k in range(K):\n",
        "    mu = gmm.means_[k]\n",
        "    cov = gmm.covariances_[k]\n",
        "    ax.contour(x_mesh, y_mesh, multivariate_normal.pdf(X_mesh, mean=mu, cov=cov))\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')"
      ],
      "metadata": {
        "id": "u5LQe4TxvK2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 平面上の各点の p(z=k|x) の値を求める\n",
        "p = gmm.predict_proba(X_mesh.reshape((-1, 2))).T\n",
        "pp = p.reshape((K, X_mesh.shape[0], X_mesh.shape[1]))\n",
        "\n",
        "# 学習データをクラスタへ hard-assign\n",
        "y = gmm.predict(X)\n",
        "mu = gmm.means_\n",
        "\n",
        "# グラフ\n",
        "fig = plt.figure(figsize=(9, 6))\n",
        "\n",
        "# 平面を p(z=k|x) の値で塗り分け\n",
        "ax0 = fig.add_subplot(121)\n",
        "cmap = ['Blues', 'Oranges', 'Greens']\n",
        "for k in range(K):\n",
        "    ax0.scatter(X[y==k, 0], X[y==k, 1])\n",
        "    ax0.contourf(x_mesh, y_mesh, pp[k], levels=[0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], cmap=cmap[k], alpha=0.3)\n",
        "    ax0.plot(mu[k, 0], mu[k, 1], color='white', marker='*', markerfacecolor=colors[k], markersize=25)\n",
        "ax0.set_xlim(xmin, xmax)\n",
        "ax0.set_ylim(ymin, ymax)\n",
        "ax0.set_aspect('equal')\n",
        "\n",
        "# 学習データ点の塗り分け\n",
        "ax1 = fig.add_subplot(122)\n",
        "colors = seaborn.color_palette(n_colors=K)\n",
        "for k in range(K):\n",
        "    ax1.scatter(X[y == k, 0], X[y == k, 1], label=f'cluster {k}')\n",
        "    ax1.plot(mu[k, 0], mu[k, 1], color='white', marker='*', markerfacecolor=colors[k], markersize=25)\n",
        "ax1.set_xlim(xmin, xmax)\n",
        "ax1.set_ylim(ymin, ymax)\n",
        "ax1.legend()\n",
        "ax1.set_aspect('equal')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_LHqGOc-waZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "左図は，平面全体を $p(z=k|\\mathbf{x})$ の値によって塗り分けたものである．右図は，学習データ点を hard-assigmnent した結果を示している．"
      ],
      "metadata": {
        "id": "zZU8IiogF-hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 主成分分析"
      ],
      "metadata": {
        "id": "pMV3UtaJHboT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次元削減の目的は，与えられたデータを，そこに含まれる情報をなるべく損なわないようにしつつ，より低次元へと変換することである．\n",
        "統計手法である **主成分分析** (Principal Component Analysis, PCA)では，次元削減の変換を線形変換によって構成する手法の一つである．\n",
        "\n",
        "以下では，$N$個の$D$次元ベクトルから成る学習データ\n",
        "\n",
        "$$\n",
        "\\{ \\mathbf{x}_n \\in {\\cal R}^{D} | n = 1, 2, \\ldots, N\\}\n",
        "$$\n",
        "\n",
        "が与えられるとして，これを $H(\\leq D)$ 次元に次元削減する場合を考える．また，話を簡単にするために，学習データの平均は $\\mathbf{0}$ である，すなわち\n",
        "\n",
        "$$\n",
        "\\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_{n} = \\mathbf{0}\n",
        "$$\n",
        "\n",
        "と仮定する（$\\mathbf{0}$でない場合の扱いについては後述する）．"
      ],
      "metadata": {
        "id": "KikotxwNIdrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### $1$次元に次元削減する場合"
      ],
      "metadata": {
        "id": "iRxo2jUIJ-aB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "［主成分分析の問題設定（1次元にする場合）］\n",
        "\n",
        "上で説明したような学習データが与えられるとする．\n",
        "このとき，大きさ $1$ の $D$ 次元ベクトル $\\mathbf{u}$ を用いて，次式によって $D$次元ベクトル $\\mathbf{x}$ を1つの実数値 $z_n$ に変換することを考える．\n",
        "\n",
        "$$\n",
        "z_n = \\mathbf{u}\\cdot \\mathbf{x}_n \\quad (n = 1, 2, \\ldots, N)\n",
        "$$\n",
        "\n",
        "ベクトル $\\mathbf{u}$ を，大きさ1($\\Vert\\mathbf{u}\\Vert = 1$)のベクトルの中で $z_n$の分散が最大となるように定めたい．\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rCIL-IZdRYLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "この問題の解，つまり，$z_n$ の分散を最大にする $\\mathbf{u}$ は，「**データ$\\mathbf{x}_n$の分散共分散行列の固有ベクトルのうち，最大の固有値に対応するもの**」となる（導出過程は後述）．\n",
        "したがって，学習データから分散共分散行列を計算し，その固有値固有ベクトルを求めれば，主成分分析による次元削減を実現できる．\n",
        "\n"
      ],
      "metadata": {
        "id": "eACBZ0j485rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "上記の解の導出過程の概略を以下に示す．"
      ],
      "metadata": {
        "id": "ftx-JdAM8-2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず，$\\mathbf{x}_n$ の平均が $\\mathbf{0}$ なので，$z_n$ の平均も $0$ となる（Q1）．そのため，$z_n$ の分散は\n",
        "\n",
        "$$\n",
        "\\frac{1}{N}\\sum_{n=1}^{N}z_n^2\n",
        "$$\n",
        "\n",
        "と表せる．このとき，\n",
        "\n",
        "$$\n",
        "\\frac{1}{N}\\sum_{n=1}^{N}z_n^2 = \\mathbf{u}^{\\top}V\\mathbf{u}\n",
        "$$\n",
        "\n",
        "と書ける（Q2）．ただし，$V$ は学習データの分散共分散行列である．\n",
        "\n"
      ],
      "metadata": {
        "id": "ae1F-ZHuK6KM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここで求めたいのは，$\\{ z_n \\}$ の分散 $\\mathbf{u}^\\top V\\mathbf{u}$ を最大にするベクトル $\\mathbf{u}$ である．ただし，$\\mathbf{u}$ には $\\Vert\\mathbf{u}\\Vert = 1$ という条件がある．このような制約条件付きの最適化問題を解く定番の手法は，「ラグランジュの未定乗数法」である（注）．\n",
        "この問題の場合，ラグランジュ乗数を $\\lambda$ として，\n",
        "\n",
        "$$\n",
        "L = \\mathbf{u}^{\\top}V\\mathbf{u} - \\lambda(\\mathbf{u}^{\\top}\\mathbf{u} - 1)\n",
        "$$\n",
        "\n",
        "とおけば，$\\frac{\\partial L}{\\partial \\mathbf{u}} = \\mathbf{0}$ を満たす $\\mathbf{u}$ が解の候補となる．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 「ラグランジュの未定乗数法」は，大学初年次の微積分で学んでいる...かも．興味のあるひとは数学の参考書を調べてね．\n",
        "</span>\n",
        "\n",
        "ここで，\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbf{u}} = 2V\\mathbf{u} - 2\\lambda\\mathbf{u}\n",
        "$$\n",
        "\n",
        "となる（注）ので，解は\n",
        "\n",
        "$$\n",
        "V\\mathbf{u} = \\lambda\\mathbf{u}\n",
        "$$\n",
        "\n",
        "を満たさねばならない．この式より，解の候補は $V$ の単位固有ベクトルであることがわかる．\n",
        "この式を $L$ の式に代入すると，\n",
        "\n",
        "$$\n",
        "L = \\lambda\\mathbf{u}^\\top\\mathbf{u} - \\lambda\\cdot 0 = \\lambda\n",
        "$$\n",
        "\n",
        "となるので，この問題の解は，$V$の最大固有値に対する固有ベクトルであることが分かる．\n",
        "\n",
        "\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: $\\frac{\\partial L}{\\partial \\mathbf{u}}$ は，「$L$を $\\mathbf{u}$ の各要素で偏微分したものをならべたベクトル」です．\n",
        "</span>"
      ],
      "metadata": {
        "id": "O7wp00oc5zgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**★★★ やってみよう ★★★**\n",
        "\n",
        "Q1, Q2 を証明しよう．"
      ],
      "metadata": {
        "id": "pGP-5pscKwGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### $2$次元以上に次元削減する場合"
      ],
      "metadata": {
        "id": "EDedrAc2NPAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "主成分分析の目的は，線形変換によって，「元のデータの分散をなるべく保つような」次元削減を行うことである（厳密な問題の定式化は省略する）．\n",
        "\n",
        "$D$ 次元ベクトル（$D\\times 1 $ 行列） $\\mathbf{x}$ を $H$ 次元ベクトル（$H\\times 1$行列） $\\mathbf{z}$  に線形変換する式を\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = W\\mathbf{x}\n",
        "$$\n",
        "\n",
        "と表すと，この式は\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = \\begin{pmatrix}\n",
        "z_1\\\\\n",
        "z_2\\\\\n",
        "\\vdots\\\\\n",
        "z_H\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "\\mathbf{w}_1\\cdot\\mathbf{x}\\\\\n",
        "\\mathbf{w}_2\\cdot\\mathbf{x}\\\\\n",
        "\\vdots\\\\\n",
        "\\mathbf{w}_H\\cdot\\mathbf{x}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "と書ける．このような線形変換 $W$ を定めることは，$H$個のベクトル $\\mathbf{w}_1, \\mathbf{w}_2,\\ldots,\\mathbf{w}_H$ を定めることと同じである．\n",
        "\n",
        "このとき，「元のデータの分散を保つ」という意味で最適な線形変換 $W$ は，次のようになる．\n",
        "\n"
      ],
      "metadata": {
        "id": "Yxozjhqfv4Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "［主成分分析の問題の解］\n",
        "\n",
        "$N$個の$D$次元ベクトルから成る学習データが与えられるとする．\n",
        "$$\n",
        "\\{ \\mathbf{x}_n \\in {\\cal R}^{D} | n = 1, 2, \\ldots, N\\}\n",
        "$$\n",
        "ただし，このデータの平均は $\\mathbf{0}$ とする．\n",
        "また，$\\mathbf{x}$ の分散共分散行列 $V = \\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_n\\mathbf{x}_n^{\\top}$ の固有値を $\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_D$とし，これらに対応する単位固有ベクトルをそれぞれ $\\mathbf{u}_1, \\mathbf{u}_2,\\ldots,\\mathbf{u}_D$ とおく．\n",
        "\n",
        "$H\\times D$行列 $W$ によって\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = W\\mathbf{x} = \\begin{pmatrix}\n",
        "\\mathbf{w}_1\\cdot\\mathbf{x}\\\\\n",
        "\\mathbf{w}_2\\cdot\\mathbf{x}\\\\\n",
        "\\vdots\\\\\n",
        "\\mathbf{w}_H\\cdot\\mathbf{x}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "と次元削減するとき，主成分分析の目的を実現するためには，\n",
        "ベクトル $\\mathbf{w}_h$ ($h = 1, 2, \\ldots, H$) を $\\mathbf{u}_h$ の向きにとればよい（注）．\n",
        "\n",
        "ここで，$\\lambda_1$ から $\\lambda_H$ （$H \\leq D$）に対応する固有ベクトル $\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_H$ をならべた，次のような行列 $U_H$ を考える．\n",
        "\n",
        "$$\n",
        "U_H = (\\mathbf{u}_1\\  \\mathbf{u}_2\\  \\ldots\\ \\mathbf{u}_H )\n",
        "$$\n",
        "\n",
        "$U_H$ は $D \\times H$ 行列である．このとき，主成分分析の変換行列 $W$ は，\n",
        "\n",
        "$$\n",
        "W = U_H^{\\top} = \\begin{pmatrix} \\mathbf{u}_1^{\\top} \\\\ \\mathbf{u}_2^{\\top} \\\\ \\vdots \\\\ \\mathbf{u}_H^{\\top} \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "となる．\n",
        "\n",
        "※ 注: ここで考えている固有ベクトルは「単位」固有ベクトル（大きさが1，すなわち $||\\mathbf{u}_h||=1$）に限定しているが，符号の不定性がある（$\\mathbf{u}_h$も$−\\mathbf{u}_h$も固有ベクトル）．したがって，$\\mathbf{w}_h = -\\mathbf{u}_h$ としてもよい．\n"
      ],
      "metadata": {
        "id": "p1y_I4eI4PfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/pcafig06b.png\">"
      ],
      "metadata": {
        "id": "MzFdZjyaD4SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習データ $\\{ \\mathbf{x}_n \\}$ の平均が $\\mathbf{0}$ でない場合の主成分分析を考える．\n",
        "この場合，データの平均を $\\mathbf{\\mu}$ として，つまり\n",
        "\n",
        "$$\n",
        "\\mathbf{\\mu} = \\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_n\n",
        "$$\n",
        "\n",
        "として，$\\mathbf{x}'_n = \\mathbf{x}_n - \\mathbf{\\mu}$ という変数変換を考えると，\n",
        "$\\mathbf{x}'_n$ の平均は $\\mathbf{0}$ である．また，その分散共分散行列 $V$ は\n",
        "\n",
        "$$\n",
        "V = \\frac{1}{N}\\sum_{n=1}^{N} \\mathbf{x}'_n{\\mathbf{x}'_n}^{\\top} = \\frac{1}{N}\\sum_{n=1}^{N} (\\mathbf{x}_n - \\mathbf{\\mu})({\\mathbf{x}_n} - \\mathbf{\\mu})^{\\top}\n",
        "$$\n",
        "\n",
        "となり，$\\mathbf{x}_n$ の分散共分散行列に等しい．したがって，$V$ の固有値と固有ベクトルを求めて，次元削減の変換を次のようにすればよい．\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = U_H^{\\top}(\\mathbf{x} - \\mathbf{\\mu}) = \\begin{pmatrix}\n",
        "\\mathbf{u}_1\\cdot(\\mathbf{x} - \\mathbf{\\mu})\\\\\n",
        "\\mathbf{u}_2\\cdot(\\mathbf{x} - \\mathbf{\\mu})\\\\\n",
        "\\vdots\\\\\n",
        "\\mathbf{u}_H\\cdot(\\mathbf{x} - \\mathbf{\\mu})\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "yxHHoIy38XsN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5jzcSMsHeCh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}