{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/AdvML/blob/2024/AdvML2024_ex05notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# AdvML ex05notebookA\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/AdvML/AdvML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?AdvML)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回の話は，学部の科目「機械学習I」でも出てきています（復習しつつ一歩先へ進む感じ）．受講していないひとは，以下をどうぞ．\n",
        "\n",
        "- 2024年度「機械学習I」 第4回 https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2024#ex04\n",
        "- 2024年度「機械学習I」 第5回 https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2024#ex05\n"
      ],
      "metadata": {
        "id": "Kh4ZRfEvArX9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ0oDxG3iygx"
      },
      "source": [
        "----\n",
        "## 準備\n",
        "----\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc  # アニメーションのため\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()"
      ],
      "metadata": {
        "id": "RjLTO96x2EvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ロジスティック回帰\n",
        "---\n"
      ],
      "metadata": {
        "id": "qrKE45oj_dUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 2クラス識別のロジスティック回帰\n"
      ],
      "metadata": {
        "id": "eMYdnEXZ_otN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$D$ 次元のデータ $\\mathbf{x} = (x_1, x_2, \\ldots, x_D)$ を2つのクラス $C_1, C_2$ に分類する問題を考える．学習データは，次の形で与えられるとする\n",
        "\n",
        "$$\n",
        "(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), (\\mathbf{x}_N, y_N)\n",
        "$$\n",
        "\n",
        "$\\mathbf{x}_n \\in {\\cal R}^{D}$ に対して，$y_n \\in \\{0, 1\\}$ はこのデータの所属クラスの正解を表す値である． $y_n = 1$ ならば $\\mathbf{x}_n$ は $C_1$ に属し，$y_n = 0$ ならば $\\mathbf{x}_n$ は $C_2$ に属すものとする（$n=1,2,\\ldots,N$）．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a9SnbqGw4tRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### モデル"
      ],
      "metadata": {
        "id": "0rQO44Uu3mWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ロジスティック回帰では，$\\mathbf{x}$ が与えられたときにそれがクラス $C_1$ に属する確率 $p(C_1|\\mathbf{x})$ を，次のようにモデル化する．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f(\\mathbf{x}) &= \\sigma(w_0 + w_1x_1+\\cdots + w_Dx_D) = \\sigma\\left(w_0 + \\sum_{d=1}^{D}w_dx_d \\right) \\\\\n",
        "&= \\frac{1}{1+\\exp{\\left( - \\left( w_0 + \\sum_{d=1}^{D}w_dx_d \\right) \\right)}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "パラメータは，$w_0, w_1, \\ldots, w_D$ の $(D+1)$ 個ある．\n",
        "\n"
      ],
      "metadata": {
        "id": "tgswEvE1_34a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "関数 $\\sigma(s)$ は **シグモイド関数** （ロジスティックシグモイド関数, sigmoid function, logistic sigmoid function）と呼ばれるものである．\n",
        "\n",
        "$$\n",
        "\\sigma(s) = \\frac{1}{1+\\exp{(-s)}}\\qquad (1)\n",
        "$$\n",
        "\n",
        "式から明らかなように，任意の実数 $s$ に対して $ 0 < \\sigma(s) < 1$ となる．この性質のため，ロジスティクス回帰モデルの出力は確率の値として解釈できる（$p(C_1|\\mathbf{x}) =  f(\\mathbf{x})$, $p(C_2|\\mathbf{x}) = 1- f(\\mathbf{x})$）．"
      ],
      "metadata": {
        "id": "riz2tH7KBc7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# シグモイド関数の値を計算\n",
        "xmin, xmax = -6, 6\n",
        "X = np.linspace(xmin, xmax, num=100)\n",
        "Y = 1/(1+np.exp(-X))\n",
        "\n",
        "# グラフに描く\n",
        "fig = plt.figure(facecolor='white')\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(-0.1, 1.1)\n",
        "ax.axhline(y=0, color='black', linestyle='-')\n",
        "ax.axvline(x=0, color='black', linestyle='-')\n",
        "ax.axhline(y=1, color='gray', linestyle='--')\n",
        "ax.plot(X, Y, linewidth=2)\n",
        "ax.set_xlabel('$s$')\n",
        "ax.set_ylabel('$\\sigma(s)$')\n",
        "#ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S_slkvSU_i6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 交差エントロピー"
      ],
      "metadata": {
        "id": "P3t4bFGR30kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f(\\mathbf{x})$ を確率モデルとみなすと，上記の学習データに対する尤度は\n",
        "\n",
        "$$\n",
        "\\prod_{n=1}^{N} f(\\mathbf{x}_n)^{y_n} \\cdot (1 - f(\\mathbf{x}_n))^{1-y_n}\n",
        "$$\n",
        "\n",
        "と表される．ロジスティック回帰モデルの学習では，この尤度の最大化を考える．\n",
        "ただし，この式の形のままでは扱いが難しいので，対数尤度\n",
        "\n",
        "$$\n",
        "\\sum_{n=1}^{N} ( y_n f(\\mathbf{x}_n) + (1 - y_n) (1 - f(\\mathbf{x}_n)))\n",
        "$$\n",
        "\n",
        "の最大化を考える．この対数尤度に負号を付けた\n",
        "\n",
        "$$\n",
        "H = -\\sum_{n=1}^{N} ( y_n f(\\mathbf{x}_n) + (1 - y_n) (1 - f(\\mathbf{x}_n)))\n",
        "$$\n",
        "\n",
        "という量は，情報理論において **交差エントロピー** (cross entropy) と呼ばれるものである．ロジスティック回帰モデルの学習は，対数尤度の最大化ともいえるし，交差エントロピーの最小化ともいえる．\n"
      ],
      "metadata": {
        "id": "4FZL27Pv8Cgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "線形回帰モデルの場合，モデルの出力と正解の値との間の二乗誤差 $E$ を最小化するパラメータ $\\mathbf{w}$ は， $\\frac{\\partial E}{\\partial \\mathbf{w}} = \\mathbf{0}$ とおいて得られる連立方程式を解くことで求まっていた．しかし，ロジスティック回帰モデルの交差エントロピー最小化の場合は，そのように簡単には解が求まらない．パラメータの初期値を適当に定めて，その値を修正して徐々に目的関数を最小化していく，逐次最適化を行う必要がある．交差エントロピーの最小化に用いることのできる最適化手法はいくつかあるが，ここでは **勾配法**，特に， **最急降下法** による解法を説明する．"
      ],
      "metadata": {
        "id": "igS3UFYyDQvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 最急降下法による学習のアルゴリズム"
      ],
      "metadata": {
        "id": "yAq8KHYPGZQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最急降下法では $H$ の $\\mathbf{w}$ に関する勾配\n",
        "\n",
        "$$\n",
        "\\nabla{H} = \\left( \\frac{\\partial H}{\\partial w_0}, \\frac{\\partial H}{\\partial w_1}, \\ldots, \\frac{\\partial H}{\\partial w_D}\\right)\n",
        "$$\n",
        "\n",
        "が必要となるので，$\\frac{\\partial H}{\\partial w_d}$ ($d=0,1,\\ldots,D$) を求めよう．\n",
        "\n",
        "ここで，$\\hat{y}_n = f(\\mathbf{x}_n)$ および\n",
        "\n",
        "$$\n",
        "\\ell_n = y_n\\log \\hat{y}_n + (1-y_n)\\log(1-\\hat{y}_n) \\qquad (3)\n",
        "$$\n",
        "\n",
        "とおくことにする．このとき，\n",
        "\n",
        "$$\n",
        "\\frac{\\partial H}{\\partial w_d} = -\\sum_{n=1}^{N}\\frac{\\partial \\ell_n}{\\partial w_d}\n",
        "$$\n",
        "\n",
        "である．"
      ],
      "metadata": {
        "id": "YBuKpChFIU5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下は，板書 + 演習の形で説明します．"
      ],
      "metadata": {
        "id": "P5vU9FpxI3ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### デモ: 2次元2クラス識別問題"
      ],
      "metadata": {
        "id": "h1AqgUYKHfxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 2次元正規分布で2クラスのデータを生成する関数\n",
        "\n",
        "def getData(seed=None):\n",
        "\n",
        "    if seed != None:\n",
        "        np.random.seed( seed )\n",
        "\n",
        "    # two 2-D spherical Gaussians\n",
        "    X0 = 1.0*np.random.randn(200, 2) + [3.0, 3.0]\n",
        "    X1 = 1.0*np.random.randn(200, 2) + [7.0, 6.0]\n",
        "    X  = np.vstack((X0, X1))\n",
        "    lab0 = np.zeros(X0.shape[0], dtype=int)\n",
        "    lab1 = np.zeros(X1.shape[0], dtype=int) + 1\n",
        "    label = np.hstack((lab0, lab1))\n",
        "\n",
        "    return X, label\n",
        "\n",
        "# データの準備\n",
        "X, lab = getData(seed=0)\n",
        "N, D = X.shape\n",
        "Yt = lab\n",
        "X = np.vstack((np.ones(N), X.T)).T\n",
        "print(f'データ数 N = {N}, 次元数 D = {D}')"
      ],
      "metadata": {
        "id": "TWdplvkZoLwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル出力の計算\n",
        "def model(X, w):\n",
        "    return 1.0 / (1.0 + np.exp(-(X @ w)))\n",
        "\n",
        "# 交差エントロピーと正解数\n",
        "def score(Y, Yt):\n",
        "    ce = -np.sum(Yt*np.log(Y)+(1.0-Yt)*np.log(1.0-Y)) # 交差エントロピー\n",
        "    count = np.sum((Y >= 0.5)*Yt) + np.sum((Y < 0.5)*(1 - Yt)) # 正解数\n",
        "    return ce, count\n",
        "\n",
        "# 勾配の計算\n",
        "def grad(X, Y, Yt):\n",
        "    return (Y - Yt) @ X"
      ],
      "metadata": {
        "id": "XM0mlpoYqMko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータの初期化\n",
        "w = (np.random.random(D+1) - 0.5) * 0.2 # [-0.1, 0.1) の一様乱数\n",
        "\n",
        "# 学習係数と学習繰り返し回数\n",
        "eta = 0.2/N\n",
        "nitr = 1000\n",
        "\n",
        "fig = plt.figure(facecolor='white', figsize=(12, 6))\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "ax2 = fig.add_subplot(122)\n",
        "elevation = 20\n",
        "azimuth = -70\n",
        "ax1.view_init(elevation, azimuth)\n",
        "ax1.set_xlim(0, 10)\n",
        "ax1.set_ylim(0, 10)\n",
        "ax1.set_zlim(0, 1)\n",
        "ax1.scatter(X[Yt==0, 1], X[Yt==0, 2], 0)\n",
        "ax1.scatter(X[Yt==1, 1], X[Yt==1, 2], 1)\n",
        "#fig.show()\n",
        "\n",
        "ax2.set_xlim(0, nitr)\n",
        "ax2.set_ylim(0, 300)\n",
        "\n",
        "aList = []\n",
        "xx, yy = np.meshgrid(np.linspace(0, 10, num=16), np.linspace(0, 10, num=16))\n",
        "xxr, yyr = xx.ravel(), yy.ravel()\n",
        "XX = np.vstack((np.ones(xxr.shape[0]), xxr, yyr)).T\n",
        "\n",
        "iList = []\n",
        "ceList = []\n",
        "\n",
        "for i in range(nitr+1):\n",
        "\n",
        "    Y = model(X, w)     # モデル出力の計算\n",
        "    ce, count = score(Y, Yt) # 交差エントロピーと正解数の計算\n",
        "    dw = grad(X, Y, Yt) # 勾配の計算\n",
        "    w -= eta * dw       # パラメータの更新\n",
        "\n",
        "    if (i < 100 and i % 10 == 0) or i % 100 == 0:\n",
        "        iList.append(i)\n",
        "        ceList.append(ce)\n",
        "        ZZ = model(XX, w)\n",
        "        zz = ZZ.reshape(xx.shape)\n",
        "        a1 = ax1.plot_wireframe(xx, yy, zz, color='green')\n",
        "        a2 = ax2.plot(iList, ceList, color='blue', marker='.')\n",
        "        rr = count/N*100\n",
        "        s = f'H = {ce:.3f}\\nrate = {rr:.1f}%'\n",
        "        a3 = ax2.text(500, 240, s, size=20)\n",
        "        aList.append([a1]+a2 + [a3])\n",
        "\n",
        "anim = animation.ArtistAnimation(fig, aList, interval=300)\n",
        "rc('animation', html='jshtml')\n",
        "plt.close()\n",
        "anim\n"
      ],
      "metadata": {
        "id": "mV8N4fqIwCiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 多クラス識別のロジスティック回帰"
      ],
      "metadata": {
        "id": "RYVDq59iKQzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**［ロジスティック回帰の問題設定（$K$クラスの場合）］**\n",
        "\n",
        "$D$次元のデータを$K$個のクラス $C_1, C_2, \\ldots, C_K$ に識別するモデルを学習させる．学習データは $N$ 個あり，次のように与えられる．\n",
        "\n",
        "$$\n",
        "(\\mathbf{x}_1, \\mathbf{y}_1), (\\mathbf{x}_2, \\mathbf{y}_2),\\ldots , (\\mathbf{x}_N, \\mathbf{y}_N)\n",
        "$$\n",
        "\n",
        "ただし，$\\mathbf{x}_n \\in {\\cal R}^{D}$ はモデルへの入力である．また，$\\mathbf{y}_n \\in \\{0, 1\\}^{K}$ （$0$か$1$のみを要素にもつ$K$次元ベクトル）はこのデータの所属クラスの正解を表す値である（$n=1,2,\\ldots,N$）．\n",
        "\n",
        "$\\mathbf{y}_n = (y_{n,1}, y_{n,2}, \\ldots, y_{n,K})$ の要素 $y_{n,k}$ は，$n$番目の学習データが $k$ 番目のクラスに所属するならば $1$，さもなくば $0$ をとる．したがって，$\\mathbf{y}_n$ の要素はどれか一つだけが $1$ で他は全て $0$ である．\n",
        "\n",
        "学習モデルは次式で定める．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{y}_k &= \\frac{\\exp s_k}{\\displaystyle\\sum_{k=1}^{K}\\exp{s_k}} \\qquad (k = 1, 2, \\ldots, K) \\qquad (*)\\\\\n",
        "s_k &= w_{k,0} + \\sum_{d=1}^{D}w_{k,d}x_d\n",
        "\\end{aligned}\n",
        "$$\n",
        "このモデルのパラメータは $w_{k,d}$ ($k = 1, 2, \\ldots, K, d = 0, 1, \\ldots, D$) の $K\\times (D+1)$ 個ある．\n",
        "\n",
        "このとき，モデルの出力と正解の値との間の「遠さ」を，次式の交差エントロピーで定義する．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "H &= -\\sum_{n=1}^{N} \\sum_{k=1}^{K} y_{n,k}\\log{\\hat{y}_{n,k}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "この $H$ の値がなるべく小さくなるようにパラメータ $\\{ w_{k,d} \\}$ を求めたい．"
      ],
      "metadata": {
        "id": "U3WrawaFAS6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "式 $(*)$ は **ソフトマックス関数**（**softmax**関数）と呼ばれるものである．\n",
        "式の形から分かるように，$0 < \\hat{y}_k < 1$ および $\\sum_{k=1}^{K}\\hat{y}_k = 1$ となる性質がある．そのため，$\\hat{y}_k$ は，$\\mathbf{x}$ という値をもつデータがクラス $C_k$ に所属する確率 $p(C_k|\\mathbf{x})$ を表すと解釈できる．"
      ],
      "metadata": {
        "id": "VvAYZ8z1LZ6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "導出過程は省略するが，上記の交差エントロピのパラメータに関する勾配は次のようになる．\n",
        "\n",
        "$$\n",
        "\\frac{\\partial H}{\\partial w_{k,d}} =  -\\sum_{n}^{N}(y_{n,k} - \\hat{y}_{n,k}) x_{n,d} \\qquad(k = 1, 2, \\ldots, K, d = 0, 1, \\ldots, D)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "HmbNSAuSKgNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 確率的勾配降下法(SGD)，バッチSGD\n",
        "---"
      ],
      "metadata": {
        "id": "KtpwrM7KN-fI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "板書して解説します．"
      ],
      "metadata": {
        "id": "jkRgEvXsPorl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 確率的勾配降下法(SGD)"
      ],
      "metadata": {
        "id": "MQFeFSjLP5F6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### バッチSGD"
      ],
      "metadata": {
        "id": "rqHh1RNrP9rF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### デモ: ロジスティック回帰による手書き数字の識別"
      ],
      "metadata": {
        "id": "Imm_P9sEQAkU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hyAGNwhe5S0"
      },
      "outputs": [],
      "source": [
        "# 手書き数字データの入手\n",
        "! wget -nc https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/minimnist.npz\n",
        "rv = np.load('minimnist.npz')\n",
        "datL = rv['datL'].astype(float)\n",
        "labL = rv['labL']\n",
        "datT = rv['datT'].astype(float)\n",
        "labT = rv['labT']\n",
        "print(datL.shape, labL.shape, datT.shape, labT.shape)\n",
        "\n",
        "K = 10 # クラス数\n",
        "\n",
        "# 学習データの用意\n",
        "NL, D = datL.shape # 学習データの数と次元数\n",
        "XL = np.empty((NL, D+1))\n",
        "XL[:, 0] = 1.0\n",
        "XL[:, 1:] = datL/255\n",
        "YtL = np.zeros((NL, K))\n",
        "for ik in range(K):\n",
        "    YtL[labL == ik, ik] = 1.0\n",
        "\n",
        "# テストデータの用意\n",
        "NT, _ = datT.shape # テストデータの数\n",
        "XT = np.empty((NT, D+1))\n",
        "XT[:, 0] = 1.0\n",
        "XT[:, 1:] = datT/255\n",
        "YtT = np.zeros((NT, K))\n",
        "for ik in range(K):\n",
        "    YtT[labT == ik, ik] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータの初期化\n",
        "W = (np.random.random((K, D+1)) - 0.5) * 0.2 # [-0.1, 0.1) の一様乱数\n",
        "\n",
        "# 学習係数と学習繰り返し回数\n",
        "eta = 0.01\n",
        "nitr = 20000\n",
        "\n",
        "# 学習\n",
        "for i in range(nitr+1):\n",
        "\n",
        "    # 学習データの一つをランダムに選択\n",
        "    n = np.random.randint(NL)\n",
        "    x, yt = XL[n, :], YtL[n]\n",
        "    # モデル出力の計算\n",
        "    exps = np.exp(W @ x)\n",
        "    y = exps / np.sum(exps)\n",
        "    # 最急降下法\n",
        "    dW = (y - yt)[:, np.newaxis] @ x[np.newaxis, :]\n",
        "    W -= eta * dW\n",
        "\n",
        "    if (i < 1000 and i % 100 == 0) or (i % 1000 == 0):\n",
        "        # モデル出力の計算\n",
        "        exps = np.exp(XL @ W.T)\n",
        "        Y = exps / np.sum(exps, axis=1)[:, np.newaxis]\n",
        "        # 交差エントロピー\n",
        "        ce = -np.sum(YtL * np.log(Y))\n",
        "        # 正解数\n",
        "        count = np.sum(labL == np.argmax(Y, axis=1))\n",
        "        print(f'{i}  {ce/NL:.3f}  {count/NL:.3f}')\n",
        "\n",
        "print()\n",
        "\n",
        "# テスト\n",
        "exps = np.exp(XT @ W.T)\n",
        "Y = exps / np.sum(exps, axis=1)[:, np.newaxis]\n",
        "ce = -np.sum(YtT * np.log(Y))\n",
        "count = np.sum(labT == np.argmax(Y, axis=1))\n",
        "print(f'テスト: {ce/NT:.3f}  {count/NT:.3f}')"
      ],
      "metadata": {
        "id": "AuxGKbnojXBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5u6OJGzdO5cG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}